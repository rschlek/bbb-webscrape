{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "1. Go to the Better Business Bureau website\n",
    "2. Locate all the listings for roofing contractors in Macon, Georgia\n",
    "3. Build a crawler in Python (use any framework/library you choose) that will extract the following\n",
    "data points for each roofing contractor, while also filtering out all “waterproof” and\n",
    "“waterproofing” contractors:\n",
    "● Company name\n",
    "● Phone number\n",
    "● Address with street, city, state, and zip code\n",
    "● Company Website (if available)\n",
    "● Email Address (if available)\n",
    "● BBB Rating\n",
    "● Accredited Date (if available)\n",
    "● Profile page URL\n",
    "● (Any other information you think is good to have/relevant)\n",
    "4. Export results in .csv format\n",
    "5. Make a new column in your results file. In that column, please rank the results based on which\n",
    "businesses you think would have the highest close rate for our sales department\n",
    "6. Write a 3-5 sentence explanation on your thought process and why you ranked the results the\n",
    "way you did\n",
    "7. Write a 3-5 sentence explanation for the following question: If you came across a website where\n",
    "there were no phone numbers on the company profiles, what alternative methods would you\n",
    "employ to find them?\n",
    "8. Email your recruiter your ranked results csv file, the crawler code you wrote to extract the data,\n",
    "and your 2 responses. The team will review your project and reach out about next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Business Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROME_DRIVER_PATH = './chromedriver_v_118.exe'\n",
    "\n",
    "SERVICE = Service(CHROME_DRIVER_PATH)\n",
    "\n",
    "CHROME_OPTIONS = Options()\n",
    "CHROME_OPTIONS.add_argument(\"--headless=new\")\n",
    "CHROME_OPTIONS.add_argument('user-agent=')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_content(url, driver, expected_content='Better Business Bureau'):\n",
    "    '''\n",
    "    Function to force the page to reload, in case there are any gateway errors host side\n",
    "    In the case of future errors with IP blocking, this is where we could change the way we are getting page content\n",
    "    '''\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    while expected_content not in driver.page_source:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "def get_business_links(search_phrase: str, accredation=False, city:str='Macon', state:str='GA'):\n",
    "    '''\n",
    "    Function to get all the links to business profile pages off of a searach on bbb.org\n",
    "    Code can be changed in future to get different cities/states as well\n",
    "    '''\n",
    "    \n",
    "    driver = webdriver.Chrome(service=SERVICE,options=CHROME_OPTIONS)\n",
    "\n",
    "    url = f'https://www.bbb.org/search?find_loc={city}%2C%20{state}&find_text={search_phrase.replace(\" \",\"%20\")}&page='\n",
    "    start_url = url + '1'\n",
    "\n",
    "    wait_for_content(start_url,driver)\n",
    "\n",
    "    # Getting past accredited prompt window, which only happens when first loading the site\n",
    "    try:\n",
    "        if not accredation:       \n",
    "            driver.find_element(By.XPATH,'//*[@id=\"root\"]/dialog[2]/form/fieldset/div[3]/div/label/span').click()\n",
    "        driver.find_element(By.XPATH,'//*[@id=\"root\"]/dialog[2]/form/div/button').click()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Declaring set to store links to businesses\n",
    "    business_links = set()\n",
    "\n",
    "    # Iterating over the listings to get all the business links\n",
    "    next_page = True\n",
    "    next_page_url = ''\n",
    "\n",
    "    while next_page:\n",
    "        if next_page_url != '':\n",
    "            wait_for_content(next_page_url,driver)\n",
    "            \n",
    "        # Identifying business cards\n",
    "        business_cards = driver.find_elements(By.XPATH,'//a[starts-with(@class, \"text-blue-medium\") and @href]')\n",
    "        \n",
    "        # Adding all data\n",
    "        for card in business_cards:\n",
    "            business_links.add(card.get_attribute('href'))\n",
    "\n",
    "        # Loading next page\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//a[@rel=\"next\"]')))\n",
    "            next_page_url = next_button.get_attribute(\"href\")\n",
    "        except Exception:\n",
    "            next_page_url = ''\n",
    "            next_page = False\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "    return business_links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: The site seems to only load 15 pages of 15 businesses at a time\n",
    "\n",
    "I am going to create a master set of business links, and search for varying keywords to try and beef up the results as much as possible. \n",
    "\n",
    "This probably isn't the best approach but for now it will do better than just taking the first search results. In the future, some ideas for other approaches could be applying different filters, sorts, or parameters to the search to get more results out of each search keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_for_links_roofing():\n",
    "    \n",
    "    # Search terms which the scraper will periodically search for\n",
    "    search_terms = [\n",
    "    \"roofing contractors\",\n",
    "    \"roofing companies\",\n",
    "    \"roofing services\",\n",
    "    \"roofers\",\n",
    "    \"roof inc\",\n",
    "    \"roofing inc\",\n",
    "    \"roofing co\",\n",
    "    \"roofing co.\",\n",
    "    \"roofing llc\"\n",
    "    \"'s roofing\",\n",
    "    \"& roofing\",\n",
    "    \"a roofing\",\n",
    "    \"b roofing\",\n",
    "    \"c roofing\",\n",
    "    \"d roofing\",\n",
    "    \"e roofing\",\n",
    "    \"f roofing\",\n",
    "    \"g roofing\",\n",
    "    \"h roofing\",\n",
    "    \"i roofing\",\n",
    "    \"j roofing\",\n",
    "    \"k roofing\",\n",
    "    \"l roofing\",\n",
    "    \"m roofing\",\n",
    "    \"n roofing\",\n",
    "    \"o roofing\",\n",
    "    \"p roofing\",\n",
    "    \"q roofing\",\n",
    "    \"r roofing\",\n",
    "    \"s roofing\",\n",
    "    \"t roofing\",\n",
    "    \"u roofing\",\n",
    "    \"v roofing\",\n",
    "    \"w roofing\",\n",
    "    \"roofing construction\",\n",
    "    \"roof repair\",\n",
    "    \"roofing specialists\",\n",
    "    \"roof installation\",\n",
    "    \"roof maintenance\",\n",
    "    \"roofing experts\",\n",
    "    \"roofing professionals\",\n",
    "    \"roofing solutions\",\n",
    "    \"commercial roofing\",\n",
    "    \"residential roofing\",\n",
    "    \"roofing repair\",\n",
    "    \"roofing experts\",\n",
    "    \"roofing pro\",\n",
    "    \"trust roofing\"\n",
    "    ]\n",
    "\n",
    "    link_set = set()\n",
    "    search_num = 1\n",
    "\n",
    "    # Iterating over the search terms to scrape for links\n",
    "    for term in search_terms:\n",
    "        curr_search = get_business_links(term)\n",
    "        acc_search = get_business_links(term,True)\n",
    "        link_set = link_set.union(curr_search).union(acc_search)\n",
    "        print(f'Current Num of Links: {len(link_set)}')\n",
    "        search_num+=1\n",
    "        \n",
    "\n",
    "    # Writes to CSV so that we can save the links over time\n",
    "    existing_links = set()\n",
    "    try:\n",
    "        with open(\"links.csv\", \"r\") as file:\n",
    "            for line in file:\n",
    "                existing_links.add(line.strip())\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    new_links_to_add = link_set - existing_links\n",
    "\n",
    "    with open(\"links.csv\", \"a\") as file:\n",
    "        for link in new_links_to_add:\n",
    "            file.write(link + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this method is not computationally optimal. With more time, Threading could be implemented to make searches run in parallel across multiple chrome drivers to speed up the data collection process. More search terms could also be added to gather more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Business Data\n",
    "\n",
    "Data points to collect:\n",
    "\n",
    "1. Company name\n",
    "2. Phone number\n",
    "3. Address (Street, City, State, Zip)\n",
    "4. Company Website\n",
    "5. Email Address - Not available (on bbb site there is a button to compose email in browser, but it does not give the actual email)\n",
    "6. BBB Rating\n",
    "7. Accredited Date\n",
    "8. Profile url (already have)\n",
    "\n",
    "Nice to haves, which will be used to calculate saleability score:\n",
    "\n",
    "9. Category\n",
    "10. Years in business\n",
    "11. Complaints last 3 years\n",
    "12. Complaints last 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_data(url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=SERVICE, options=CHROME_OPTIONS)\n",
    "        wait_for_content(url, driver)\n",
    "\n",
    "        # Initialize default values\n",
    "        data = {\n",
    "            \"name\": None,\n",
    "            \"street\": None,\n",
    "            \"city\": None,\n",
    "            \"state\": None,\n",
    "            \"zipcode\": None,\n",
    "            \"phone\": None,\n",
    "            \"website\": None,\n",
    "            \"bbb_url\": url,\n",
    "            \"category\": None,\n",
    "            \"accredited\": None,\n",
    "            \"bbb_rating\": None,\n",
    "            \"accredited_date\": None,\n",
    "            \"years_in_business\": None,\n",
    "            \"one_year_complaints\": 0,\n",
    "            \"three_year_complaints\": 0\n",
    "        }\n",
    "\n",
    "        # Company Name\n",
    "        try:\n",
    "            data[\"name\"] = driver.find_element(By.XPATH, '//*[@id=\"content\"]/div[1]/div/header/div/div/h1/span[3]').text\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Phone Number\n",
    "        try:\n",
    "            data[\"phone\"] = driver.find_element(By.XPATH,'//a[contains(@href, \"tel:\")][1]').text\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Address\n",
    "        try:\n",
    "            address_elements = driver.find_element(By.XPATH, '//div[starts-with(@class, \"card stack dtm-contact\")]').find_element(By.XPATH, '//div[starts-with(@class, \"dtm-address\")]').text.split('\\n')\n",
    "            # Initialize variables\n",
    "            street_address = None\n",
    "            city = None\n",
    "            state = None\n",
    "            zipcode = None\n",
    "\n",
    "            # Case if there is a suit number 3rd line in the address    \n",
    "            if len(address_elements) > 2 and address_elements[2] != 'Get Directions':\n",
    "                address_elements[0] += ', ' + address_elements[1]\n",
    "                address_elements.pop(1)\n",
    "\n",
    "            if len(address_elements) > 1:\n",
    "                street_address = address_elements[0]\n",
    "                city_state_zip = address_elements[1]\n",
    "            else:\n",
    "                city_state_zip = address_elements[0]\n",
    "\n",
    "            # Split the city/state/zip into separate components\n",
    "            city, state_zipcode = city_state_zip.split(\",\", 1)\n",
    "            state_zipcode = state_zipcode.strip()\n",
    "            state, zipcode = state_zipcode.split(\" \", 1)\n",
    "\n",
    "            data['street'] = street_address\n",
    "            data['city'] = city\n",
    "            data['state'] = state\n",
    "            data['zipcode'] = zipcode\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Website\n",
    "        try:\n",
    "            data[\"website\"] = driver.find_element(By.CSS_SELECTOR, '.card.dtm-contact .dtm-url').get_attribute('href')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # BBB rating\n",
    "        try:\n",
    "            data[\"bbb_rating\"] = driver.find_element(By.XPATH, '//*[starts-with(@class, \"dtm-rating\")]').text.split('\\n')[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Accredation status\n",
    "        accredited = True\n",
    "        try:\n",
    "            acc_status = driver.find_element(By.XPATH, '//*[@id=\"content\"]/div[2]/div[2]/div[2]/div[2]/div/div[1]/a').text\n",
    "            if 'BBB' in acc_status:\n",
    "                accredited = False\n",
    "                data[\"accredited\"] = 'Not Accredited'\n",
    "        except Exception:\n",
    "            data['accredited'] = 'Accredited'\n",
    "\n",
    "        # Accredited date\n",
    "        if accredited:\n",
    "            try:\n",
    "                data[\"accredited_date\"] = driver.find_element(By.XPATH,'//p[contains(., \"Accredited Since\")]').text.split(': ')[1]\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Category\n",
    "        try:\n",
    "            category = url.split('/')[7].replace(\"-\", \" \").replace(\"contractors\", \"Contractors\").title()\n",
    "            data['category'] = category\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Years in business\n",
    "        try:\n",
    "            data[\"years_in_business\"] = int(driver.find_element(By.XPATH, '//p[contains(., \"Years in Business\")]').text.split(': ')[1])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Complaints\n",
    "        try:\n",
    "            complaints = driver.find_element(By.XPATH, '//h3[contains(text(), \"Customer Complaints\")]/following::div[1]').text\n",
    "            if 'closed' in complaints:\n",
    "                \n",
    "                numbers = re.findall(r'(\\d+)\\s+complaints', complaints)\n",
    "\n",
    "                data[\"three_year_complaints\"] = numbers[0]\n",
    "                data[\"one_year_complaints\"] = numbers[1]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "    finally:\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(csv_file):\n",
    "    results = []\n",
    "\n",
    "    # Read the links from the CSV file\n",
    "    try:\n",
    "        with open(csv_file, 'r') as file:\n",
    "            links = [line.strip() for line in file]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{csv_file}' not found.\")\n",
    "        return\n",
    "\n",
    "    total_links = len(links)\n",
    "    # Progress threshold for notification (25% intervals)\n",
    "    progress_threshold = total_links // 4 \n",
    "\n",
    "    def scrape_data(link):\n",
    "        curr_data = get_business_data(link)\n",
    "        if curr_data:\n",
    "            results.append(curr_data)\n",
    "\n",
    "        if len(results) % progress_threshold == 0:\n",
    "            progress = len(results) / total_links * 100\n",
    "            print(f\"Progress: {progress:.2f}% ({len(results)} links processed)\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        for link in links:\n",
    "            executor.submit(scrape_data, link)\n",
    "\n",
    "    pd.DataFrame(results).to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataframe('links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_saleability_score(row):\n",
    "    '''\n",
    "    Function to calculate saleability score. Note that these values are estimates, and\n",
    "    a more comprehensive model could be later developed to refine the weighting of different factors\n",
    "\n",
    "    The max score is 100%\n",
    "\n",
    "    bbb rating is worth 25%. \n",
    "    This rating varies from NR, F to A+ and is based on the better business bureau's own rating\n",
    "    The thought process is that a higher rated business has longer track record of fewer complaints and more transparency.\n",
    "    These businesses likely care about their customers, and can be sold on reaching more.\n",
    "    https://www.bbb.org/overview-of-bbb-ratings\n",
    "\n",
    "    accredited date is worth 10%. \n",
    "    The bbb rating is a good indicator, but not if it was given out yesterday. \n",
    "    Every year under 5 years that the accredation was received, the business loses 2% score\n",
    "\n",
    "    years in business is worth 10%.\n",
    "    same as accredited date, longer standing businesses will likely be a good indicator of overall business quality\n",
    "\n",
    "    contact info completeness is worth 15%\n",
    "    in order to sell and close, we need contact info, and the more the better.\n",
    "    each piece that is missing is worth 3% (street,city,state,zipcode,phone)\n",
    "\n",
    "    website is worth 10%\n",
    "    if no website, it is harder to prepare for a sales call, making it harder to close\n",
    "\n",
    "    category is worth 15%\n",
    "    the category we are interested in is \n",
    "\n",
    "    there is also 0.10 available for not having any complaints in last year, and 0.05 for the last three years \n",
    "    all businesses automatically receieve these points if they have no complaints\n",
    "    '''\n",
    "    \n",
    "    weights = {\n",
    "    \"bbb_rating\": 0.25,\n",
    "    \"accredited_date\": 0.10,\n",
    "    \"years_in_business\": 0.10,\n",
    "    \"contact_info_completeness\": 0.15,\n",
    "    \"website\": 0.10,\n",
    "    \"category\": 0.15,\n",
    "    \"one_year_complaints\": 0.10,\n",
    "    \"three_year_complaints\": 0.05,\n",
    "    } \n",
    "    \n",
    "    score = 0\n",
    "\n",
    "    # BBB Rating\n",
    "    bbb_mapping = {\n",
    "        'A+': 1.0,\n",
    "        'A': 0.94,\n",
    "        'A-': 0.90,\n",
    "        'B+': 0.87,\n",
    "        'B': 0.84,\n",
    "        'B-': 0.80,\n",
    "        'C+': 0.77,\n",
    "        'C': 0.74,\n",
    "        'C-': 0.70,\n",
    "        'D+': 0.67,\n",
    "        'D': 0.64,\n",
    "        'D-': 0.60,\n",
    "        'F': 0.0,\n",
    "        'NR': 0.0\n",
    "    }\n",
    "    #bbb score\n",
    "    bbb_score = bbb_mapping.get(row['bbb_rating'],0) * weights['bbb_rating']\n",
    "\n",
    "    # Time since accredition\n",
    "    accredited_score = 0\n",
    "    if row['accredited']=='Accredited':\n",
    "        years_ago = (pd.Timestamp.now() - pd.Timestamp(row['accredited_date'])).days // 365\n",
    "        accredited_score = min(years_ago,5)/5*weights['accredited_date']\n",
    "\n",
    "    # years in business\n",
    "    time_in_business = 0\n",
    "    if not pd.isna(row['years_in_business']):\n",
    "        time_in_business = min(row['years_in_business'],10)/10*weights['years_in_business']\n",
    "\n",
    "    three_year = 0\n",
    "    # complaints\n",
    "    if row['three_year_complaints']==0:\n",
    "        three_year = weights['three_year_complaints']\n",
    "\n",
    "    one_year = 0\n",
    "    if row['one_year_complaints']==0:\n",
    "        one_year = weights['one_year_complaints']\n",
    "\n",
    "    # Website\n",
    "    website_score = 0\n",
    "    if not pd.isna(row['website']):\n",
    "        website_score = weights['website']\n",
    "\n",
    "    category = 0\n",
    "    if row['category']=='Roofing Contractors':\n",
    "        category = weights['category']\n",
    "\n",
    "\n",
    "    #Contact info completeness\n",
    "    factors = [row['name'],row['street'],row['city'],row['state'],row['zipcode'],row['phone']]\n",
    "    total_factors = len(factors)\n",
    "    total_present = 0\n",
    "    for factor in factors:\n",
    "        if not pd.isna(factor):\n",
    "            total_present += 1\n",
    "    completeness = total_present/total_factors*weights['contact_info_completeness']\n",
    "\n",
    "\n",
    "    # final score\n",
    "    score = completeness + category + website_score + one_year + three_year + time_in_business + accredited_score + bbb_score\n",
    "\n",
    "    return round(score,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results.csv',index_col=0)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"saleability_score\"] = df.apply(calculate_saleability_score, axis=1)\n",
    "\n",
    "# Dropping waterproofing businesses\n",
    "drop_keywords = ['water','waterproof','waterproofing']\n",
    "df['name_lower'] = df['name'].str.lower()\n",
    "df = df[~(df['name_lower'].str.contains('|'.join(drop_keywords), na=False) & pd.notna(df['name_lower']))]\n",
    "df = df.drop(columns='name_lower')\n",
    "\n",
    "# Sorting by saleability\n",
    "df = df.sort_values(by=\"saleability_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('scored_result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
